<div class="hero is-small is-info is-bold">
    <div class="hero-body">
        <nav class="columns is-vcentered">
        <!-- Left side -->
        <div class="column">
            <div class="container">
                <h1 class="title">GAZER</h1>
                <h2 class="subtitle">
                    Gaze-assisted apps.
                </h2>
                <div class="links">
                    <a href="https://github.com/CowboyLynk/Gazer/" target="_blank">
                        <i class="fab fa-github fa-md"></i>
                        Github
                    </a>
                    |
                    <a href="https://www.youtube.com/watch?v=HTb9IbW29a4" target="_blank">
                        <i class="fab fa-youtube fa-md"></i>
                        YouTube
                    </a>
                </div>
            </div>
        </div>

        <!-- Right side -->
        <div class="column">
            <figure class="image">
            <img id="hero-image" src="images/gazer/hero.png">
        </figure>
        </div>
        </nav>
    </div>
</div>
<section class="section" id="project_info">
    <div class="container">
        <h2 class="subtitle is-4">About</h2>
        <p>
            Gazer is a multifunctional gaze-assisted voice-command menu framework made for iOS apps. It works by using Apple’s ARKit framework to track a user’s gaze across the device. When the estimated gaze intersects with the menu’s speech icon in the upper-left-hand corner of the screen, the menu activates, expands, and begins accepting voice commands. Speech processing happens asynchronously using Apple’s Speech framework that, when finished, calls a function defined by the developer. The menu and it’s corresponding UI element are abstracted and require very little setup. As such, they can be used in a variety of use cases that respond differently to the same commands depending on the current view.
        </p>
        <p>
            The system works quite well for estimating the general position of a user’s gaze, but is very sensitive to noise. Even when staring at a fixed location, the estimated gaze position varies an average of 50 pixels in any direction. Furthermore, even after calibration, the gaze estimation is far less accurate at the bottom of the screen than at the top.
        </p>

        <h2 class="subtitle is-4">Video</h2>
        <div class="videoWrapper">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/HTb9IbW29a4?start=39" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
    </div>
</section>